{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihSTqc-XgJeM"
   },
   "source": [
    "#STEP 1 — Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN-vfuqgftx6",
    "outputId": "7701a97d-0bbe-479b-b26d-29133e913f39"
   },
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install transformers\n",
    "!pip install soundfile\n",
    "!pip install torch torchaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FupYUKGYJsZm"
   },
   "source": [
    "#Multimodal Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkH2R7xQiRQY"
   },
   "source": [
    "#SPEECH MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-kV4iuCiQoi",
    "outputId": "df342a60-3352-4f91-adb1-90b6aec6c7c2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PI1l7K7fibCs",
    "outputId": "f64a4081-1bde-4082-a4b7-00824e8ef1b7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQ8spZDG4dij"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_path = \"/content/drive/MyDrive/TESS Toronto emotional speech set data\"\n",
    "\n",
    "emotion_files = {}\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "\n",
    "        if file.endswith(\".wav\"):\n",
    "\n",
    "            emotion = file.split(\"_\")[-1].replace(\".wav\",\"\")\n",
    "\n",
    "            if emotion not in emotion_files:\n",
    "                emotion_files[emotion] = []\n",
    "\n",
    "            emotion_files[emotion].append(os.path.join(folder_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTC0OO_U6iiA",
    "outputId": "cc14e420-c6c1-470b-854c-7b4e5404354a"
   },
   "outputs": [],
   "source": [
    "train_paths = []\n",
    "train_labels = []\n",
    "\n",
    "test_paths = []\n",
    "test_labels = []\n",
    "\n",
    "emotion_map = {\n",
    "    \"angry\":0,\n",
    "    \"disgust\":1,\n",
    "    \"fear\":2,\n",
    "    \"happy\":3,\n",
    "    \"pleasant_surprise\":4,\n",
    "    \"ps\":4,\n",
    "    \"sad\":5,\n",
    "    \"neutral\":6\n",
    "}\n",
    "\n",
    "for emotion, files in emotion_files.items():\n",
    "\n",
    "    train_files, test_files = train_test_split(\n",
    "        files,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    train_paths.extend(train_files)\n",
    "    test_paths.extend(test_files)\n",
    "\n",
    "    train_labels.extend([emotion_map[emotion]] * len(train_files))\n",
    "    test_labels.extend([emotion_map[emotion]] * len(test_files))\n",
    "\n",
    "print(\"Train samples:\", len(train_paths))\n",
    "print(\"Test samples:\", len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YWYxIHI7FvH",
    "outputId": "1b931550-3932-498f-dd0a-9b21e18c0cdd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_root = \"/content/drive/MyDrive/New_Project_pipeline\"\n",
    "\n",
    "hubert_train_dir = os.path.join(project_root, \"hubert_embeddings_train\")\n",
    "hubert_test_dir  = os.path.join(project_root, \"hubert_embeddings_test\")\n",
    "\n",
    "os.makedirs(hubert_train_dir, exist_ok=True)\n",
    "os.makedirs(hubert_test_dir, exist_ok=True)\n",
    "\n",
    "print(\"Project structure ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "40bdaea258f5462eb5da98fed1601fc2",
      "8476c11da525450cbab914f493a61628",
      "20170c498a33414198cbe5b4d443e293",
      "9ade6a2a132742a8be3fae9fd3698daf",
      "62d64c5612a4491baff12d35881e69c9",
      "a4a94a110bf44d3c8f59d9e8ccab0d72",
      "8e8725cb01364d82a241bbfb9ae28864",
      "6dc49a8fb0fe46d78262a4bd931ce1b4",
      "d415dde8866443cb8f37e038bd06a3c3",
      "d871484a462e44e1aa8ce1d68c6f485d",
      "e4c9fb66bd07476bb03e9a3680add3ac"
     ]
    },
    "id": "6tmUnD7NwY0H",
    "outputId": "e14e5782-1c56-487b-d135-3d644debd98a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "hubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\").to(device)\n",
    "hubert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmQLpnJZwdoL"
   },
   "outputs": [],
   "source": [
    "def extract_hubert(audio_path):\n",
    "\n",
    "    speech, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        speech,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    input_values = inputs.input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = hubert_model(input_values)\n",
    "\n",
    "    embedding = outputs.last_hidden_state.squeeze(0)  # [T,768]\n",
    "\n",
    "    return embedding.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UHl9CUj7ZG8"
   },
   "source": [
    "#Train Hubert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTCl4BYiwj9J",
    "outputId": "ba4e079d-9ae6-4619-8941-97a4804bb587"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "corrupted = []\n",
    "\n",
    "for path in tqdm(train_paths):\n",
    "\n",
    "    try:\n",
    "        emb = extract_hubert(path)\n",
    "\n",
    "        file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "        torch.save(emb, os.path.join(hubert_train_dir, file_name))\n",
    "\n",
    "    except:\n",
    "        corrupted.append(path)\n",
    "\n",
    "print(\"Corrupted train files:\", len(corrupted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Etq7W95KB0HS"
   },
   "source": [
    "#Test Hubert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JePhCUu7hvR",
    "outputId": "5b96c1c6-e025-4164-f1be-fb7070162064"
   },
   "outputs": [],
   "source": [
    "corrupted_test = []\n",
    "\n",
    "for path in tqdm(test_paths):\n",
    "\n",
    "    try:\n",
    "        emb = extract_hubert(path)\n",
    "\n",
    "        file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "        torch.save(emb, os.path.join(hubert_test_dir, file_name))\n",
    "\n",
    "    except:\n",
    "        corrupted_test.append(path)\n",
    "\n",
    "print(\"Corrupted test files:\", len(corrupted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZxxBcw3AWnt",
    "outputId": "1ece5179-9145-46de-f2ec-42213af5d47c"
   },
   "outputs": [],
   "source": [
    "len(os.listdir(hubert_train_dir))\n",
    "len(os.listdir(hubert_test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTL-n3MuwfuB"
   },
   "outputs": [],
   "source": [
    "def load_train_embedding(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(hubert_train_dir, file_name))\n",
    "\n",
    "def load_test_embedding(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(hubert_test_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1pbZVJgxss2"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionBiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(256, 7)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "\n",
    "        x = x.unsqueeze(0)            # [1,T,768]\n",
    "        out, _ = self.lstm(x)         # [1,T,256]\n",
    "        pooled = out.mean(dim=1)      # [1,256]\n",
    "\n",
    "        if return_features:\n",
    "            return pooled\n",
    "\n",
    "        output = self.fc(pooled)      # [1,7]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hcbma-dayHQk",
    "outputId": "ab6bc604-580b-41b7-db98-746b72e0d8b6"
   },
   "outputs": [],
   "source": [
    "model = EmotionBiLSTM().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for path, label in zip(train_paths, train_labels):\n",
    "\n",
    "        features = load_train_embedding(path).to(device)\n",
    "        label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, label_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_losses.append(total_loss)\n",
    "    print(f\"Epoch {epoch+1} Loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s65-d8L2BQns"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\n",
    "           \"/content/drive/MyDrive/New_Project_pipeline/final_hubert_bilstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfgoLi3cBRPx",
    "outputId": "3b10a0db-41b4-4b4e-e6b7-fb7f215c4c15"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "for path in test_paths:\n",
    "\n",
    "    features = load_test_embedding(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "\n",
    "    pred = torch.argmax(outputs, dim=1).item()\n",
    "    preds.append(pred)\n",
    "\n",
    "acc = accuracy_score(test_labels, preds)\n",
    "\n",
    "print(\"Speech Accuracy:\", acc)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUb7bJC8BTP6",
    "outputId": "614a4e7c-c02d-4c42-8574-3725dab9a8ae"
   },
   "outputs": [],
   "source": [
    "speech_train_pooled_dir = \"/content/drive/MyDrive/New_Project_pipeline/speech_train_pooled\"\n",
    "speech_test_pooled_dir  = \"/content/drive/MyDrive/New_Project_pipeline/speech_test_pooled\"\n",
    "\n",
    "os.makedirs(speech_train_pooled_dir, exist_ok=True)\n",
    "os.makedirs(speech_test_pooled_dir, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Train pooled\n",
    "for path in tqdm(train_paths):\n",
    "\n",
    "    features = load_train_embedding(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pooled = model(features, return_features=True)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\",\".pt\")\n",
    "    torch.save(pooled.cpu(),\n",
    "               os.path.join(speech_train_pooled_dir, file_name))\n",
    "\n",
    "# Test pooled\n",
    "for path in tqdm(test_paths):\n",
    "\n",
    "    features = load_test_embedding(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pooled = model(features, return_features=True)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\",\".pt\")\n",
    "    torch.save(pooled.cpu(),\n",
    "               os.path.join(speech_test_pooled_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bZEPlt2-EZK1",
    "outputId": "e9691057-231e-493b-c8f2-d1ef69c4369c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# =========================\n",
    "# SET RESULT DIRECTORY\n",
    "# =========================\n",
    "results_dir = \"/content/drive/MyDrive/New_Project_pipeline/Results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "emotion_names = [\n",
    "    \"angry\",\n",
    "    \"disgust\",\n",
    "    \"fear\",\n",
    "    \"happy\",\n",
    "    \"pleasant_surprise\",\n",
    "    \"sad\",\n",
    "    \"neutral\"\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# EVALUATION\n",
    "# =========================\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "temporal_features = []\n",
    "\n",
    "for path, label in zip(test_paths, test_labels):\n",
    "\n",
    "    features = load_test_embedding(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pooled = model(features, return_features=True)\n",
    "        outputs = model(features)\n",
    "\n",
    "    pred = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(label)\n",
    "    temporal_features.append(pooled.cpu().numpy())\n",
    "\n",
    "temporal_features = np.vstack(temporal_features)\n",
    "\n",
    "# =========================\n",
    "# ACCURACY\n",
    "# =========================\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(\"Speech Accuracy:\", accuracy)\n",
    "\n",
    "# Save accuracy table\n",
    "accuracy_df = pd.DataFrame({\n",
    "    \"Model\": [\"Speech (HuBERT + BiLSTM)\"],\n",
    "    \"Accuracy\": [accuracy]\n",
    "})\n",
    "\n",
    "accuracy_df.to_csv(os.path.join(results_dir, \"speech_accuracy.csv\"), index=False)\n",
    "\n",
    "# =========================\n",
    "# CLASSIFICATION REPORT\n",
    "# =========================\n",
    "report = classification_report(all_labels, all_preds, target_names=emotion_names)\n",
    "\n",
    "print(report)\n",
    "\n",
    "with open(os.path.join(results_dir, \"speech_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# =========================\n",
    "# CONFUSION MATRIX\n",
    "# =========================\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=emotion_names,\n",
    "            yticklabels=emotion_names)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - Speech Model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"speech_confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# =============================\n",
    "# Normalize embeddings first\n",
    "# =============================\n",
    "scaler = StandardScaler()\n",
    "temporal_features_norm = scaler.fit_transform(temporal_features)\n",
    "\n",
    "# =============================\n",
    "# t-SNE (improved parameters)\n",
    "# =============================\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate=200,\n",
    "    n_iter=2000,\n",
    "    random_state=42,\n",
    "    init=\"pca\"\n",
    ")\n",
    "\n",
    "tsne_2d = tsne.fit_transform(temporal_features_norm)\n",
    "\n",
    "# =============================\n",
    "# Plot\n",
    "# =============================\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "palette = sns.color_palette(\"tab10\", len(emotion_names))\n",
    "\n",
    "for i, emo in enumerate(emotion_names):\n",
    "    idx = np.where(np.array(all_labels) == i)\n",
    "    plt.scatter(\n",
    "        tsne_2d[idx,0],\n",
    "        tsne_2d[idx,1],\n",
    "        label=emo,\n",
    "        s=45\n",
    "    )\n",
    "\n",
    "plt.title(\"Temporal Modelling Representation (BiLSTM)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"speech_tsne.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"All results saved inside:\", results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZecZDxrEd33"
   },
   "source": [
    "#TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzjTtD2aEiG8",
    "outputId": "8e460b19-a6b1-4e8d-cbbf-9b3649f8b2e9"
   },
   "outputs": [],
   "source": [
    "def build_text_dataset(paths):\n",
    "    texts = []\n",
    "    for path in paths:\n",
    "        file = path.split(\"/\")[-1]\n",
    "        word = file.split(\"_\")[1]\n",
    "        sentence = f\"say the word {word}\"\n",
    "        texts.append(sentence)\n",
    "    return texts\n",
    "\n",
    "train_texts = build_text_dataset(train_paths)\n",
    "test_texts  = build_text_dataset(test_paths)\n",
    "\n",
    "print(train_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "097ec2ef15fa48eabc27d43e8dcb6d9e",
      "eac7ce89424f44a88ad6a55a84e626a3",
      "85e48f807eaf4aa295811d0669f340f0",
      "1ab9e8b0d4a647a7b05fa13d2ba88f72",
      "50961f268f784dd98de1ac6efac22d15",
      "d1a5b43bc14943038579843f52f902e1",
      "c9a074690dc64104be85c8217b656818",
      "2e645355800246869b4ca1240cd1b644",
      "bde58f4c38394ef0912fe036bc849b0e",
      "7e5296f798444eb68bd4978dc0434eca",
      "e9f2e8558f7d4ce3a0dfbc7a9d6c71d0"
     ]
    },
    "id": "nOfD6K5PFTAZ",
    "outputId": "7289b268-554a-4193-ccc9-ce6fd02a652d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbJvF_XkFTiv"
   },
   "outputs": [],
   "source": [
    "project_root = \"/content/drive/MyDrive/New_Project_pipeline\"\n",
    "\n",
    "text_train_dir = os.path.join(project_root, \"bert_embeddings_train\")\n",
    "text_test_dir  = os.path.join(project_root, \"bert_embeddings_test\")\n",
    "\n",
    "os.makedirs(text_train_dir, exist_ok=True)\n",
    "os.makedirs(text_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17bOMhgtFaww",
    "outputId": "3901ddab-3943-4564-d8eb-5065bcc7b7ed"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_cls(text):\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=16\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    cls_embedding = outputs.last_hidden_state[:,0,:]  # [1,768]\n",
    "\n",
    "    return cls_embedding.cpu()\n",
    "\n",
    "\n",
    "# Save TRAIN embeddings\n",
    "for text, path in tqdm(zip(train_texts, train_paths), total=len(train_texts)):\n",
    "\n",
    "    emb = extract_cls(text)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    torch.save(emb, os.path.join(text_train_dir, file_name))\n",
    "\n",
    "\n",
    "# Save TEST embeddings\n",
    "for text, path in tqdm(zip(test_texts, test_paths), total=len(test_texts)):\n",
    "\n",
    "    emb = extract_cls(text)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    torch.save(emb, os.path.join(text_test_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoibvcUWFoFW"
   },
   "outputs": [],
   "source": [
    "def load_text_train(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_train_dir, file_name))\n",
    "\n",
    "def load_text_test(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_test_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhU_3EE9FrRf"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextEmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMVVZ6HmFwk2",
    "outputId": "84b6f5c1-9bcc-4f89-de2f-303038272049"
   },
   "outputs": [],
   "source": [
    "text_model = TextEmotionClassifier().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(text_model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 30\n",
    "epoch_losses_text = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    text_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for path, label in zip(train_paths, train_labels):\n",
    "\n",
    "        emb = load_text_train(path).to(device)\n",
    "        label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "        preds = text_model(emb)\n",
    "        loss = criterion(preds, label_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_losses_text.append(total_loss)\n",
    "    print(f\"Epoch {epoch+1} Loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rgD3ACWiF0ZF",
    "outputId": "d904255e-ab0f-473b-a554-0d3b5e9dcae6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "results_dir = \"/content/drive/MyDrive/New_Project_pipeline/Results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "emotion_names = [\n",
    "    \"angry\",\n",
    "    \"disgust\",\n",
    "    \"fear\",\n",
    "    \"happy\",\n",
    "    \"pleasant_surprise\",\n",
    "    \"sad\",\n",
    "    \"neutral\"\n",
    "]\n",
    "\n",
    "text_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "context_features = []\n",
    "\n",
    "for path, label in zip(test_paths, test_labels):\n",
    "\n",
    "    emb = load_text_test(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = text_model(emb)\n",
    "\n",
    "    pred = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(label)\n",
    "    context_features.append(emb.cpu().numpy())\n",
    "\n",
    "context_features = np.vstack(context_features)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(\"Text Accuracy:\", accuracy)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Model\": [\"Text (BERT CLS)\"],\n",
    "    \"Accuracy\": [accuracy]\n",
    "}).to_csv(os.path.join(results_dir, \"text_accuracy.csv\"), index=False)\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(all_labels, all_preds, target_names=emotion_names)\n",
    "print(report)\n",
    "\n",
    "with open(os.path.join(results_dir, \"text_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=emotion_names,\n",
    "            yticklabels=emotion_names)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - Text Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"text_confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# =============================\n",
    "# Normalize CLS embeddings\n",
    "# =============================\n",
    "scaler = StandardScaler()\n",
    "context_features_norm = scaler.fit_transform(context_features)\n",
    "\n",
    "# =============================\n",
    "# t-SNE with improved settings\n",
    "# =============================\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate=200,\n",
    "    n_iter=2000,\n",
    "    random_state=42,\n",
    "    init=\"pca\"\n",
    ")\n",
    "\n",
    "tsne_2d = tsne.fit_transform(context_features_norm)\n",
    "\n",
    "# =============================\n",
    "# Plot\n",
    "# =============================\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for i, emo in enumerate(emotion_names):\n",
    "    idx = np.where(np.array(all_labels) == i)\n",
    "    plt.scatter(\n",
    "        tsne_2d[idx, 0],\n",
    "        tsne_2d[idx, 1],\n",
    "        label=emo,\n",
    "        s=45\n",
    "    )\n",
    "\n",
    "plt.title(\"Contextual Modelling Representation (BERT CLS)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"text_tsne.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Text pipeline results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LflwsHZmwNk1"
   },
   "source": [
    "#FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8pEemC2Kp7O"
   },
   "outputs": [],
   "source": [
    "project_root = \"/content/drive/MyDrive/New_Project_pipeline\"\n",
    "\n",
    "speech_train_pooled_dir = os.path.join(project_root, \"speech_train_pooled\")\n",
    "speech_test_pooled_dir  = os.path.join(project_root, \"speech_test_pooled\")\n",
    "\n",
    "text_train_dir = os.path.join(project_root, \"bert_embeddings_train\")\n",
    "text_test_dir  = os.path.join(project_root, \"bert_embeddings_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEHdBpKKKtFz"
   },
   "outputs": [],
   "source": [
    "def load_speech_train(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(speech_train_pooled_dir, file_name))\n",
    "\n",
    "def load_speech_test(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(speech_test_pooled_dir, file_name))\n",
    "\n",
    "def load_text_train(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_train_dir, file_name))\n",
    "\n",
    "def load_text_test(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_test_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaaljAX2KvCR"
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class FusionEmotionModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.fc1 = nn.Linear(1024, 7)\n",
    "#         # self.relu = nn.ReLU()\n",
    "#         # self.fc2 = nn.Linear(256, 7)\n",
    "\n",
    "#     def forward(self, speech_emb, text_emb):\n",
    "\n",
    "#         fused = torch.cat((speech_emb, text_emb), dim=1)  # [1,1024]\n",
    "#         x = self.fc1(fused)\n",
    "#         # x = self.relu(x)\n",
    "#         # x = self.fc2(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FusionEmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Direct linear mapping\n",
    "        self.fc = nn.Linear(1024, 7)\n",
    "\n",
    "    def forward(self, speech_emb, text_emb, return_features=False):\n",
    "\n",
    "        fused = torch.cat((speech_emb, text_emb), dim=1)  # [1,1024]\n",
    "\n",
    "        if return_features:\n",
    "            return fused  # raw fused representation\n",
    "\n",
    "        output = self.fc(fused)  # [1,7]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFiVQp3BKyts",
    "outputId": "e0243a39-9d99-42c1-c534-c674c5946adb"
   },
   "outputs": [],
   "source": [
    "fusion_model = FusionEmotionModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=3e-4)\n",
    "\n",
    "epochs = 20\n",
    "epoch_losses_fusion = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    fusion_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for path, label in zip(train_paths, train_labels):\n",
    "\n",
    "        speech_emb = load_speech_train(path)\n",
    "        text_emb   = load_text_train(path)\n",
    "\n",
    "        if speech_emb is None or text_emb is None:\n",
    "            continue\n",
    "\n",
    "        speech_emb = speech_emb.to(device)\n",
    "        text_emb   = text_emb.to(device)\n",
    "\n",
    "        label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "        preds = fusion_model(speech_emb, text_emb)\n",
    "        loss = criterion(preds, label_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_losses_fusion.append(total_loss)\n",
    "    print(f\"Epoch {epoch+1} Loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ji-RibiLDmq",
    "outputId": "50b263ce-4958-4268-a653-46187c4e22a0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "fusion_model.eval()\n",
    "\n",
    "fusion_preds = []\n",
    "fusion_labels = []\n",
    "fusion_features = []\n",
    "\n",
    "for path, label in zip(test_paths, test_labels):\n",
    "\n",
    "    speech_emb = load_speech_test(path)\n",
    "    text_emb   = load_text_test(path)\n",
    "\n",
    "    if speech_emb is None or text_emb is None:\n",
    "        continue\n",
    "\n",
    "    speech_emb = speech_emb.to(device)\n",
    "    text_emb   = text_emb.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = fusion_model(speech_emb, text_emb)\n",
    "\n",
    "    pred = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    fusion_preds.append(pred)\n",
    "    fusion_labels.append(label)\n",
    "\n",
    "    fused_vector = torch.cat((speech_emb, text_emb), dim=1)\n",
    "    fusion_features.append(fused_vector.cpu().numpy())\n",
    "\n",
    "fusion_features = np.vstack(fusion_features)\n",
    "\n",
    "acc_fusion = accuracy_score(fusion_labels, fusion_preds)\n",
    "\n",
    "print(\"Fusion Accuracy:\", acc_fusion)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(fusion_labels, fusion_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTbI2a1fLEP0"
   },
   "outputs": [],
   "source": [
    "results_dir = \"/content/drive/MyDrive/New_Project_pipeline/Results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Accuracy table\n",
    "pd.DataFrame({\n",
    "    \"Model\": [\"Fusion (Speech + Text)\"],\n",
    "    \"Accuracy\": [acc_fusion]\n",
    "}).to_csv(os.path.join(results_dir, \"fusion_accuracy.csv\"), index=False)\n",
    "\n",
    "# Classification report\n",
    "with open(os.path.join(results_dir, \"fusion_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(classification_report(fusion_labels, fusion_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N2psmycLJcX"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(fusion_labels, fusion_preds)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=emotion_names,\n",
    "            yticklabels=emotion_names)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Fusion Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"fusion_confusion_matrix.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "guNh7IKkyHgg",
    "outputId": "81ec3629-0a71-44eb-ee0a-fe269258ef0d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# emotion names\n",
    "emotion_names = [\"angry\",\"disgust\",\"fear\",\"happy\",\"ps\",\"sad\",\"neutral\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1 — Normalize features\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "fusion_features_norm = scaler.fit_transform(fusion_features)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2 — PCA (very important)\n",
    "# reduces noise from 1024 → 50\n",
    "# -----------------------------\n",
    "pca = PCA(n_components=50)\n",
    "fusion_pca = pca.fit_transform(fusion_features_norm)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3 — t-SNE\n",
    "# tuned for emotion embeddings\n",
    "# -----------------------------\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=35,\n",
    "    learning_rate=150,\n",
    "    n_iter=3000,\n",
    "    random_state=42,\n",
    "    init=\"pca\",\n",
    "    metric=\"cosine\"\n",
    ")\n",
    "\n",
    "fusion_2d = tsne.fit_transform(fusion_pca)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4 — Plot class clusters\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for i, emo in enumerate(emotion_names):\n",
    "    idx = np.where(np.array(fusion_labels) == i)\n",
    "    plt.scatter(\n",
    "        fusion_2d[idx,0],\n",
    "        fusion_2d[idx,1],\n",
    "        label=emo,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "plt.title(\"t-SNE — Fusion Representation (Speech + Text)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"fusion_tsne_final.png\"), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJ7u2dm4zELG"
   },
   "outputs": [],
   "source": [
    "fusion_logits = []\n",
    "fusion_labels = []\n",
    "\n",
    "fusion_model.eval()\n",
    "\n",
    "for path, label in zip(test_paths, test_labels):\n",
    "\n",
    "    speech_emb = load_speech_test(path)\n",
    "    text_emb   = load_text_test(path)\n",
    "\n",
    "    if speech_emb is None or text_emb is None:\n",
    "        continue\n",
    "\n",
    "    speech_emb = speech_emb.to(device)\n",
    "    text_emb   = text_emb.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = fusion_model(speech_emb, text_emb)\n",
    "\n",
    "    fusion_logits.append(logits.cpu().numpy())\n",
    "    fusion_labels.append(label)\n",
    "\n",
    "fusion_logits = np.vstack(fusion_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "ycV2hViKzFX8",
    "outputId": "c3e4f89c-ef8e-41db-f37f-7d0230aeea0c"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "logits_norm = scaler.fit_transform(fusion_logits)\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    n_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "fusion_2d = tsne.fit_transform(logits_norm)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for i, emo in enumerate(emotion_names):\n",
    "    idx = np.where(np.array(fusion_labels) == i)\n",
    "    plt.scatter(fusion_2d[idx,0], fusion_2d[idx,1], label=emo, s=50)\n",
    "\n",
    "plt.title(\"Fusion Decision Space (Logits) t-SNE\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(results_dir, \"fusion_tsne.png\"), dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
