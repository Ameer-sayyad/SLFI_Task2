{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FK2K0Dq0ukKZ",
    "outputId": "eb92d4c1-e378-4f19-b62b-32e9d53899ac"
   },
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install transformers\n",
    "!pip install soundfile\n",
    "!pip install torch torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRvxR0JuuxIU",
    "outputId": "8e822e7d-244d-46a0-b65c-03db838c9b65"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qysuTw_gE9ci"
   },
   "source": [
    "#Speech Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cE_xwqWouzIt",
    "outputId": "6d1ac560-9157-499e-deb0-ca1ad908e21f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwRZ2LKazV0m"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_path = \"/content/drive/MyDrive/TESS Toronto emotional speech set data\"\n",
    "\n",
    "emotion_files = {}\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "\n",
    "        if file.endswith(\".wav\"):\n",
    "\n",
    "            emotion = file.split(\"_\")[-1].replace(\".wav\",\"\")\n",
    "\n",
    "            if emotion not in emotion_files:\n",
    "                emotion_files[emotion] = []\n",
    "\n",
    "            emotion_files[emotion].append(os.path.join(folder_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNifqP8OzWI6",
    "outputId": "fc4ff934-26eb-4a0c-b8e6-4f428b3f63ec"
   },
   "outputs": [],
   "source": [
    "train_paths = []\n",
    "train_labels = []\n",
    "\n",
    "val_paths = []\n",
    "val_labels = []\n",
    "\n",
    "test_paths = []\n",
    "test_labels = []\n",
    "\n",
    "emotion_map = {\n",
    "    \"angry\":0,\n",
    "    \"disgust\":1,\n",
    "    \"fear\":2,\n",
    "    \"happy\":3,\n",
    "    \"pleasant_surprise\":4,\n",
    "    \"ps\":4,\n",
    "    \"sad\":5,\n",
    "    \"neutral\":6\n",
    "}\n",
    "\n",
    "for emotion, files in emotion_files.items():\n",
    "\n",
    "    train_files, temp_files = train_test_split(\n",
    "        files,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_files, test_files = train_test_split(\n",
    "        temp_files,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    train_paths.extend(train_files)\n",
    "    val_paths.extend(val_files)\n",
    "    test_paths.extend(test_files)\n",
    "\n",
    "    train_labels.extend([emotion_map[emotion]] * len(train_files))\n",
    "    val_labels.extend([emotion_map[emotion]] * len(val_files))\n",
    "    test_labels.extend([emotion_map[emotion]] * len(test_files))\n",
    "\n",
    "print(\"Train samples:\", len(train_paths))\n",
    "print(\"Validation samples:\", len(val_paths))\n",
    "print(\"Test samples:\", len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0ruorIjzWdg",
    "outputId": "7c00c536-414f-4e1c-9ea0-52f3aa1f26c4"
   },
   "outputs": [],
   "source": [
    "project_root = \"/content/drive/MyDrive/New_Project_pipeline2\"\n",
    "\n",
    "hubert_train_dir = os.path.join(project_root, \"hubert_embeddings_train\")\n",
    "hubert_val_dir   = os.path.join(project_root, \"hubert_embeddings_val\")\n",
    "hubert_test_dir  = os.path.join(project_root, \"hubert_embeddings_test\")\n",
    "\n",
    "os.makedirs(hubert_train_dir, exist_ok=True)\n",
    "os.makedirs(hubert_val_dir, exist_ok=True)\n",
    "os.makedirs(hubert_test_dir, exist_ok=True)\n",
    "\n",
    "print(\"New Project structure ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "45faad37a5b941a08ba0ab85bfacf021",
      "4b5b382dfd3641a0b967c64308aaef6e",
      "091859054756489f8a8085792d3eaf7d",
      "a0013d6687e54025acb005bb29ca65a5",
      "3f0d824585bb4fba9d0e8f0d320328db",
      "85157d2bae204cebbb2130de95e43796",
      "90484c0b56b441f5a014c1d2628ce990",
      "a7736872d19744b7913934ce6c86102e",
      "1379c9a40296423393d85f61ec5f6936",
      "54687dfe3c3642369c744ee12f7d7dbf",
      "c3ef571e4dff40dd86aec579b2be4dcd",
      "1970e33fae534fbba9e54053f426eb92",
      "8622a3763ab74d919b921ca1e43083e6",
      "4d36693fd63c4b349cbc23729b1dd110",
      "09a10f061c264b4f87c3d60bc360b6d2",
      "92d2c57a7e394845a6ffddb587434f6c",
      "656dab6a2e254753b5c2448f5b81328b",
      "a93f94fa62b94f408f5c6c04923a319e",
      "0972c1d6f72d4ee69c3009bd5f7ba196",
      "e856b56d9af7495297dcb99e8ce0517f",
      "50117b8d9bd443b3a79ab35521a8bde9",
      "1a2d53e1d8ea43ac9a07a10f4d370753",
      "e9093b12540442fd90309bce15f3f748",
      "6f24d6db16da431db0f23c09f20ef77d",
      "97bee6a8405748a59af2f08bed577a2c",
      "b007d075902f4f818f25de40c8d8ee84",
      "650b560043f341038fcd3db3dc463cf1",
      "6fe42b91a87f4d9cb8945f7f602beec7",
      "57173ceabb6441dcb9f1e8b2c6974e7e",
      "9e8515a3a2964e7c8948a579ac84e2df",
      "14af79049be9415797c1b181e5975053",
      "0973423a2846406297f9d0347d9cf396",
      "bbacbdb87a0e4348b731a2f48ae9bfca",
      "3b1d987d893d4df2a84ce1ef58c43d94",
      "d2fd61eba2094cb2a2a7d74404aa908f",
      "9945358dcd964e32819847f63a047d14",
      "5856ad90ecdf459db3ac6f7980d03541",
      "e794316d62624c04b09bcc5d8bffc9d9",
      "8941a935e0204f0d80a553b3f7503d3e",
      "1211a05a97e0452286a70f841c1c561b",
      "d30a6b37214649a891a40e8ada1dc799",
      "0a05dc20c39b42718dbe2bec9efe0fb1",
      "718843aacc2349459747e92198432b0f",
      "c97e0c3e7b10407da8bde0367e70380c",
      "7a0d36017ac84c90a4858a884ac662aa",
      "b277e0a302334cbfb177f96c9dc9fe0f",
      "91b447baba7f440f88b3a59beddc1a7e",
      "aa683def478847c98de7400cefd84042",
      "67eea1145a1f4969b9b2573cfb7ac5d6",
      "a759a028771a4d05a08ac2b9a0beff15",
      "2f8796c739a64950848017cc87b71d9e",
      "4bb6d4726b6445478282d40f6b3c267c",
      "79435e7dd27d46159a58b188ec44495e",
      "9348fd0e024a4f909f45e9eee351714a",
      "5a59ac7328c04a9a8f2764d041548e27"
     ]
    },
    "id": "235_VG8hzWwf",
    "outputId": "ee19b4bb-7351-4520-c554-7a7d4d8c8e11"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "hubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\").to(device)\n",
    "hubert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyUEfGJ7zW8g"
   },
   "outputs": [],
   "source": [
    "def extract_hubert(audio_path):\n",
    "\n",
    "    speech, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        speech,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    input_values = inputs.input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = hubert_model(input_values)\n",
    "\n",
    "    embedding = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    return embedding.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVNxVTpFzdhU",
    "outputId": "e24168f4-f8c7-42e8-b545-31fc20f710ae"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "corrupted = []\n",
    "\n",
    "for path in tqdm(train_paths):\n",
    "\n",
    "    try:\n",
    "        emb = extract_hubert(path)\n",
    "\n",
    "        file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "        torch.save(emb, os.path.join(hubert_train_dir, file_name))\n",
    "\n",
    "    except:\n",
    "        corrupted.append(path)\n",
    "\n",
    "print(\"Corrupted train files:\", len(corrupted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zjPGJLqzd0D",
    "outputId": "7a4d8d24-f3b6-4e0f-a9c8-fb2c954f14bb"
   },
   "outputs": [],
   "source": [
    "corrupted_val = []\n",
    "\n",
    "for path in tqdm(val_paths):\n",
    "\n",
    "    try:\n",
    "        emb = extract_hubert(path)\n",
    "\n",
    "        file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "        torch.save(emb, os.path.join(hubert_val_dir, file_name))\n",
    "\n",
    "    except:\n",
    "        corrupted_val.append(path)\n",
    "\n",
    "print(\"Corrupted val files:\", len(corrupted_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xL7z-_FzeCY",
    "outputId": "89a00c47-f18a-4832-d6c1-cdc38d3bd3e4"
   },
   "outputs": [],
   "source": [
    "corrupted_test = []\n",
    "\n",
    "for path in tqdm(test_paths):\n",
    "\n",
    "    try:\n",
    "        emb = extract_hubert(path)\n",
    "\n",
    "        file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "        torch.save(emb, os.path.join(hubert_test_dir, file_name))\n",
    "\n",
    "    except:\n",
    "        corrupted_test.append(path)\n",
    "\n",
    "print(\"Corrupted test files:\", len(corrupted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ULF5UUozeP6"
   },
   "outputs": [],
   "source": [
    "def load_train_embedding(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(hubert_train_dir, file_name))\n",
    "\n",
    "def load_val_embedding(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(hubert_val_dir, file_name))\n",
    "\n",
    "def load_test_embedding(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(hubert_test_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mz-HYpIXz0Br"
   },
   "outputs": [],
   "source": [
    "class EmotionBiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(256, 7)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "\n",
    "        x = x.unsqueeze(0)\n",
    "        out, _ = self.lstm(x)\n",
    "        pooled = out.mean(dim=1)\n",
    "\n",
    "        if return_features:\n",
    "            return pooled\n",
    "\n",
    "        output = self.fc(pooled)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXeFh_q8z0Rj",
    "outputId": "7c932fb0-f7cb-488f-9ff1-09b96179451b"
   },
   "outputs": [],
   "source": [
    "model = EmotionBiLSTM().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "epoch_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for path, label in zip(train_paths, train_labels):\n",
    "\n",
    "        features = load_train_embedding(path).to(device)\n",
    "        label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, label_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_losses.append(total_loss)\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for path, label in zip(val_paths, val_labels):\n",
    "\n",
    "            features = load_val_embedding(path).to(device)\n",
    "            label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, label_tensor)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    val_losses.append(total_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss:.4f} | Val Loss: {total_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDBLrzUQ0HIV"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\n",
    "           \"/content/drive/MyDrive/New_Project_pipeline2/final_hubert_bilstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nh7il-eT0Js7",
    "outputId": "43e68860-93be-4e72-8be2-4d6d263bedd4"
   },
   "outputs": [],
   "source": [
    "speech_train_pooled_dir = \"/content/drive/MyDrive/New_Project_pipeline2/speech_train_pooled\"\n",
    "speech_val_pooled_dir   = \"/content/drive/MyDrive/New_Project_pipeline2/speech_val_pooled\"\n",
    "speech_test_pooled_dir  = \"/content/drive/MyDrive/New_Project_pipeline2/speech_test_pooled\"\n",
    "\n",
    "os.makedirs(speech_train_pooled_dir, exist_ok=True)\n",
    "os.makedirs(speech_val_pooled_dir, exist_ok=True)\n",
    "os.makedirs(speech_test_pooled_dir, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ======================\n",
    "# Train pooled\n",
    "# ======================\n",
    "for path in tqdm(train_paths):\n",
    "\n",
    "    features = load_train_embedding(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pooled = model(features, return_features=True)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\",\".pt\")\n",
    "    torch.save(pooled.cpu(),\n",
    "               os.path.join(speech_train_pooled_dir, file_name))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Validation pooled\n",
    "# ======================\n",
    "for path in tqdm(val_paths):\n",
    "\n",
    "    features = load_val_embedding(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pooled = model(features, return_features=True)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\",\".pt\")\n",
    "    torch.save(pooled.cpu(),\n",
    "               os.path.join(speech_val_pooled_dir, file_name))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Test pooled\n",
    "# ======================\n",
    "for path in tqdm(test_paths):\n",
    "\n",
    "    features = load_test_embedding(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pooled = model(features, return_features=True)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\",\".pt\")\n",
    "    torch.save(pooled.cpu(),\n",
    "               os.path.join(speech_test_pooled_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vyomnGCL1Aof",
    "outputId": "05d42045-f8ff-4a37-bfc4-36046b788ec1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# =========================\n",
    "# SET RESULT DIRECTORY\n",
    "# =========================\n",
    "results_dir = \"/content/drive/MyDrive/New_Project_pipeline2/Results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "emotion_names = [\n",
    "    \"angry\",\n",
    "    \"disgust\",\n",
    "    \"fear\",\n",
    "    \"happy\",\n",
    "    \"pleasant_surprise\",\n",
    "    \"sad\",\n",
    "    \"neutral\"\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# EVALUATION\n",
    "# =========================\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "temporal_features = []\n",
    "\n",
    "for path, label in zip(test_paths, test_labels):\n",
    "\n",
    "    features = load_test_embedding(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pooled = model(features, return_features=True)\n",
    "        outputs = model(features)\n",
    "\n",
    "    pred = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(label)\n",
    "    temporal_features.append(pooled.cpu().numpy())\n",
    "\n",
    "temporal_features = np.vstack(temporal_features)\n",
    "\n",
    "# =========================\n",
    "# ACCURACY\n",
    "# =========================\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(\"Speech Accuracy:\", accuracy)\n",
    "\n",
    "# Save accuracy table\n",
    "accuracy_df = pd.DataFrame({\n",
    "    \"Model\": [\"Speech (HuBERT + BiLSTM)\"],\n",
    "    \"Accuracy\": [accuracy]\n",
    "})\n",
    "\n",
    "accuracy_df.to_csv(os.path.join(results_dir, \"speech_accuracy.csv\"), index=False)\n",
    "\n",
    "# =========================\n",
    "# CLASSIFICATION REPORT\n",
    "# =========================\n",
    "report = classification_report(all_labels, all_preds, target_names=emotion_names)\n",
    "\n",
    "print(report)\n",
    "\n",
    "with open(os.path.join(results_dir, \"speech_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# =========================\n",
    "# CONFUSION MATRIX\n",
    "# =========================\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=emotion_names,\n",
    "            yticklabels=emotion_names)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - Speech Model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"speech_confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# =============================\n",
    "# Normalize embeddings first\n",
    "# =============================\n",
    "scaler = StandardScaler()\n",
    "temporal_features_norm = scaler.fit_transform(temporal_features)\n",
    "\n",
    "# =============================\n",
    "# t-SNE (improved parameters)\n",
    "# =============================\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate=200,\n",
    "    n_iter=2000,\n",
    "    random_state=42,\n",
    "    init=\"pca\"\n",
    ")\n",
    "\n",
    "tsne_2d = tsne.fit_transform(temporal_features_norm)\n",
    "\n",
    "# =============================\n",
    "# Plot\n",
    "# =============================\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "palette = sns.color_palette(\"tab10\", len(emotion_names))\n",
    "\n",
    "for i, emo in enumerate(emotion_names):\n",
    "    idx = np.where(np.array(all_labels) == i)\n",
    "    plt.scatter(\n",
    "        tsne_2d[idx,0],\n",
    "        tsne_2d[idx,1],\n",
    "        label=emo,\n",
    "        s=45\n",
    "    )\n",
    "\n",
    "plt.title(\"Temporal Modelling Representation (BiLSTM)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"speech_tsne.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"All results saved inside:\", results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKLoeH2N-VBC"
   },
   "source": [
    "#Text Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ui1IV-Eo-YSp",
    "outputId": "ec1f9997-fed3-40ca-c09c-223c4c98631d"
   },
   "outputs": [],
   "source": [
    "def build_text_dataset(paths):\n",
    "    texts = []\n",
    "    for path in paths:\n",
    "        file = path.split(\"/\")[-1]\n",
    "        word = file.split(\"_\")[1]\n",
    "        sentence = f\"say the word {word}\"\n",
    "        texts.append(sentence)\n",
    "    return texts\n",
    "\n",
    "train_texts = build_text_dataset(train_paths)\n",
    "val_texts   = build_text_dataset(val_paths)\n",
    "test_texts  = build_text_dataset(test_paths)\n",
    "\n",
    "print(train_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6781c85141f246e5b9befd064a3b0588",
      "63bbd171d53c48919371d362da1202f1",
      "86132587b6fe4d4dbb8113c023f60179",
      "5cbdd2d9b4694a2787a19187c25fb016",
      "55775a65d6554222ac704e1037697a65",
      "70ecae10098441479c327a8b3388526a",
      "71ca5512d47e48dfbdc77b47f0756c7b",
      "fb3908b1d28f43fe94b34b46a831b772",
      "53fdcf200c804b3baffa1c9ab6fa3b41",
      "60f2828a8bee4ec0b25baf550e3fed3f",
      "7bce73be768e466da43952041146669d",
      "2ddd828e158c4c8faec7923baef3ba63",
      "6b0cc42faed6408fb095009f814cf60b",
      "fa10fb1d96744c208779894d4cff44e6",
      "40d2b42b82474f6c822c8e6b24a69ac0",
      "0ba285c1d6214a74abc4559563a0de1d",
      "612a48aa9ba64eb9997d1a972e099444",
      "dc4f6ba592b34e36b7c18d14f767a5a2",
      "e736f95e109a439284aa75585c23b97a",
      "778e2d0ba1ae4b78b0702f4e77194442",
      "5205aa415713474bb5d77ea4712b9efb",
      "f666630af524422280f093022db52c3d",
      "6c970f63a6964251a6ece224af1f99d2",
      "d0826b63b6e748beb8ede5113797dafc",
      "c49d152eceb94caea5b571c17c74776b",
      "59860c6b650b43acb25badcb7e59a913",
      "6353c3398e744b3587adb8ca61cf49ed",
      "888331b095774fbca12eb595f36e5b73",
      "3efbb6b132f7420bbab664e9399b3974",
      "228dd7ab2e80493792d3efaa1a767bb1",
      "1d4688001fee42ee9ec53d2887f065bb",
      "b96b03921404459a9a5b92e52dd04d50",
      "d4b29b4d3ec541d28b15f12518fe769e",
      "c8d2c0fe07fa4d13981969bf16fbd8df",
      "b20e282a726c46539742dfc824f81cae",
      "97eaa41ce3ba4bf281bb70f04ab7dd77",
      "72d41a60e83d42eeac5011dab992258f",
      "9329e21dcfce4b98b54b8108dc55f9d9",
      "ccc18a9e9ac742d09b63aa459428aed9",
      "47982e31056c410fba2cea27c5ff5fa2",
      "f601ef96fb924609a099acbfd113195c",
      "2e8190de543446148194c8f7fbc6ca0c",
      "77d661563d184b28a6ffe8b241d6f6d5",
      "86a212501f3740338c1e2844bb033f7f",
      "cb4c8d3461cc4f55830572bbe0ddcb0d",
      "ad75a35a859648abb68d60d1c75bdea4",
      "a319a11ca141419cae7b6e55ae04f126",
      "53c6841d5dfd484e84ea06360607c34e",
      "258cc64ecddc424e8e41de5ae9b666f1",
      "c7db0466ca2a4aeda3e034561acf08fc",
      "a3a72f3845bd4eaa81598ac2400a27eb",
      "fb10697bea9d40b7b2390a3a055e721b",
      "09e2d740b12a497fb56eb1a446ba8bcc",
      "2c8b06aa25a44faebf7fb98cb63ceddb",
      "67dab1a3e73c46f09e41bab8204e6610",
      "db16fbc1da254817b029699779c836ed",
      "e08935c437934a059f261fb9cf9aa3a6",
      "ea911db7ea90478695b21e3d9cc3d414",
      "9dad269f80b6415c8347c0776ea7928e",
      "1f70aaf16b7c4ccf8cc7cca846f0aa13",
      "8b2235661df34cc9889100e897c192f9",
      "80ed1a11f0dd494f87164ecb2e97f546",
      "4323a610b8cb40bcb11be516f7c01c03",
      "b68464f98c6c4da6be03861fb90cd848",
      "f2b9cc207ad54f019f35bf5a87dd4409",
      "03987218e04a45118b7cff0505440998"
     ]
    },
    "id": "l-8PHpL9_GUt",
    "outputId": "0655b772-b360-42d3-f9a8-31e98287ac4d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXRR7a_E_I0n"
   },
   "outputs": [],
   "source": [
    "project_root = \"/content/drive/MyDrive/New_Project_pipeline2\"\n",
    "\n",
    "text_train_dir = os.path.join(project_root, \"bert_embeddings_train\")\n",
    "text_val_dir   = os.path.join(project_root, \"bert_embeddings_val\")\n",
    "text_test_dir  = os.path.join(project_root, \"bert_embeddings_test\")\n",
    "\n",
    "os.makedirs(text_train_dir, exist_ok=True)\n",
    "os.makedirs(text_val_dir, exist_ok=True)\n",
    "os.makedirs(text_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lI1sQRk_Kgb",
    "outputId": "0102e1de-2acf-46e3-da4f-baad35262a13"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_cls(text):\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=16\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    cls_embedding = outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "    return cls_embedding.cpu()\n",
    "\n",
    "\n",
    "# ======================\n",
    "# TRAIN embeddings\n",
    "# ======================\n",
    "for text, path in tqdm(zip(train_texts, train_paths), total=len(train_texts)):\n",
    "\n",
    "    emb = extract_cls(text)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    torch.save(emb, os.path.join(text_train_dir, file_name))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# VALIDATION embeddings\n",
    "# ======================\n",
    "for text, path in tqdm(zip(val_texts, val_paths), total=len(val_texts)):\n",
    "\n",
    "    emb = extract_cls(text)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    torch.save(emb, os.path.join(text_val_dir, file_name))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# TEST embeddings\n",
    "# ======================\n",
    "for text, path in tqdm(zip(test_texts, test_paths), total=len(test_texts)):\n",
    "\n",
    "    emb = extract_cls(text)\n",
    "\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    torch.save(emb, os.path.join(text_test_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q_8kcKX_PRY"
   },
   "outputs": [],
   "source": [
    "def load_text_train(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_train_dir, file_name))\n",
    "\n",
    "def load_text_val(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_val_dir, file_name))\n",
    "\n",
    "def load_text_test(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_test_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gS6l2_3-_P9I"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextEmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rclzwL3t_RW9",
    "outputId": "a2fcc033-4f99-4ce6-e1e5-a2d14933f3a5"
   },
   "outputs": [],
   "source": [
    "text_model = TextEmotionClassifier().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(text_model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 20\n",
    "epoch_losses_text = []\n",
    "val_losses_text = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ======================\n",
    "    # TRAIN\n",
    "    # ======================\n",
    "    text_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for path, label in zip(train_paths, train_labels):\n",
    "\n",
    "        emb = load_text_train(path).to(device)\n",
    "        label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "        preds = text_model(emb)\n",
    "        loss = criterion(preds, label_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_losses_text.append(total_loss)\n",
    "\n",
    "    # ======================\n",
    "    # VALIDATION\n",
    "    # ======================\n",
    "    text_model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for path, label in zip(val_paths, val_labels):\n",
    "\n",
    "            emb = load_text_val(path).to(device)\n",
    "            label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "            preds = text_model(emb)\n",
    "            loss = criterion(preds, label_tensor)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    val_losses_text.append(total_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss:.4f} | Val Loss: {total_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VabOEmqW_RuJ",
    "outputId": "7d9a8e6d-4102-43d4-eae3-a963cc55fb9b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "results_dir = \"/content/drive/MyDrive/New_Project_pipeline2/Results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "emotion_names = [\n",
    "    \"angry\",\n",
    "    \"disgust\",\n",
    "    \"fear\",\n",
    "    \"happy\",\n",
    "    \"pleasant_surprise\",\n",
    "    \"sad\",\n",
    "    \"neutral\"\n",
    "]\n",
    "\n",
    "text_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "context_features = []\n",
    "\n",
    "for path, label in zip(test_paths, test_labels):\n",
    "\n",
    "    emb = load_text_test(path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = text_model(emb)\n",
    "\n",
    "    pred = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(label)\n",
    "    context_features.append(emb.cpu().numpy())\n",
    "\n",
    "context_features = np.vstack(context_features)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(\"Text Accuracy:\", accuracy)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Model\": [\"Text (BERT CLS)\"],\n",
    "    \"Accuracy\": [accuracy]\n",
    "}).to_csv(os.path.join(results_dir, \"text_accuracy.csv\"), index=False)\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(all_labels, all_preds, target_names=emotion_names)\n",
    "print(report)\n",
    "\n",
    "with open(os.path.join(results_dir, \"text_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=emotion_names,\n",
    "            yticklabels=emotion_names)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - Text Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"text_confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# =============================\n",
    "# Normalize CLS embeddings\n",
    "# =============================\n",
    "scaler = StandardScaler()\n",
    "context_features_norm = scaler.fit_transform(context_features)\n",
    "\n",
    "# =============================\n",
    "# t-SNE with improved settings\n",
    "# =============================\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate=200,\n",
    "    n_iter=2000,\n",
    "    random_state=42,\n",
    "    init=\"pca\"\n",
    ")\n",
    "\n",
    "tsne_2d = tsne.fit_transform(context_features_norm)\n",
    "\n",
    "# =============================\n",
    "# Plot\n",
    "# =============================\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for i, emo in enumerate(emotion_names):\n",
    "    idx = np.where(np.array(all_labels) == i)\n",
    "    plt.scatter(\n",
    "        tsne_2d[idx, 0],\n",
    "        tsne_2d[idx, 1],\n",
    "        label=emo,\n",
    "        s=45\n",
    "    )\n",
    "\n",
    "plt.title(\"Contextual Modelling Representation (BERT CLS)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"text_tsne.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Text pipeline results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XclPlGVeA-x0"
   },
   "source": [
    "#Fusion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDDdlkTF_SGG"
   },
   "outputs": [],
   "source": [
    "project_root = \"/content/drive/MyDrive/New_Project_pipeline2\"\n",
    "\n",
    "speech_train_pooled_dir = os.path.join(project_root, \"speech_train_pooled\")\n",
    "speech_val_pooled_dir   = os.path.join(project_root, \"speech_val_pooled\")\n",
    "speech_test_pooled_dir  = os.path.join(project_root, \"speech_test_pooled\")\n",
    "\n",
    "text_train_dir = os.path.join(project_root, \"bert_embeddings_train\")\n",
    "text_val_dir   = os.path.join(project_root, \"bert_embeddings_val\")\n",
    "text_test_dir  = os.path.join(project_root, \"bert_embeddings_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsQyXIi1BEla"
   },
   "outputs": [],
   "source": [
    "def load_speech_train(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(speech_train_pooled_dir, file_name))\n",
    "\n",
    "def load_speech_val(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(speech_val_pooled_dir, file_name))\n",
    "\n",
    "def load_speech_test(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(speech_test_pooled_dir, file_name))\n",
    "\n",
    "\n",
    "def load_text_train(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_train_dir, file_name))\n",
    "\n",
    "def load_text_val(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_val_dir, file_name))\n",
    "\n",
    "def load_text_test(path):\n",
    "    file_name = path.split(\"/\")[-1].replace(\".wav\", \".pt\")\n",
    "    return torch.load(os.path.join(text_test_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ngMRyxHBFyh"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FusionEmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(1024, 7)\n",
    "\n",
    "    def forward(self, speech_emb, text_emb, return_features=False):\n",
    "\n",
    "        fused = torch.cat((speech_emb, text_emb), dim=1)\n",
    "\n",
    "        if return_features:\n",
    "            return fused\n",
    "\n",
    "        output = self.fc(fused)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x39JcsFfBFr_",
    "outputId": "2cfdeb1c-b671-4cb6-d2e4-5e22ef5538f1"
   },
   "outputs": [],
   "source": [
    "fusion_model = FusionEmotionModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=3e-4)\n",
    "\n",
    "epochs = 20\n",
    "epoch_losses_fusion = []\n",
    "val_losses_fusion = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ======================\n",
    "    # TRAIN\n",
    "    # ======================\n",
    "    fusion_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for path, label in zip(train_paths, train_labels):\n",
    "\n",
    "        speech_emb = load_speech_train(path)\n",
    "        text_emb   = load_text_train(path)\n",
    "\n",
    "        if speech_emb is None or text_emb is None:\n",
    "            continue\n",
    "\n",
    "        speech_emb = speech_emb.to(device)\n",
    "        text_emb   = text_emb.to(device)\n",
    "\n",
    "        label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "        preds = fusion_model(speech_emb, text_emb)\n",
    "        loss = criterion(preds, label_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_losses_fusion.append(total_loss)\n",
    "\n",
    "    # ======================\n",
    "    # VALIDATION\n",
    "    # ======================\n",
    "    fusion_model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for path, label in zip(val_paths, val_labels):\n",
    "\n",
    "            speech_emb = load_speech_val(path)\n",
    "            text_emb   = load_text_val(path)\n",
    "\n",
    "            if speech_emb is None or text_emb is None:\n",
    "                continue\n",
    "\n",
    "            speech_emb = speech_emb.to(device)\n",
    "            text_emb   = text_emb.to(device)\n",
    "\n",
    "            label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "            preds = fusion_model(speech_emb, text_emb)\n",
    "            loss = criterion(preds, label_tensor)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    val_losses_fusion.append(total_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss:.4f} | Val Loss: {total_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmAXce6EBFmy",
    "outputId": "991c5aef-5a89-4fa1-9bab-3ae225947a06"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "fusion_model.eval()\n",
    "\n",
    "fusion_preds = []\n",
    "fusion_labels = []\n",
    "fusion_features = []\n",
    "\n",
    "for path, label in zip(test_paths, test_labels):\n",
    "\n",
    "    speech_emb = load_speech_test(path)\n",
    "    text_emb   = load_text_test(path)\n",
    "\n",
    "    if speech_emb is None or text_emb is None:\n",
    "        continue\n",
    "\n",
    "    speech_emb = speech_emb.to(device)\n",
    "    text_emb   = text_emb.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = fusion_model(speech_emb, text_emb)\n",
    "\n",
    "    pred = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    fusion_preds.append(pred)\n",
    "    fusion_labels.append(label)\n",
    "\n",
    "    fused_vector = torch.cat((speech_emb, text_emb), dim=1)\n",
    "    fusion_features.append(fused_vector.cpu().numpy())\n",
    "\n",
    "fusion_features = np.vstack(fusion_features)\n",
    "\n",
    "acc_fusion = accuracy_score(fusion_labels, fusion_preds)\n",
    "\n",
    "print(\"Fusion Accuracy:\", acc_fusion)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(fusion_labels, fusion_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9M9zZHiXD81V"
   },
   "outputs": [],
   "source": [
    "results_dir = \"/content/drive/MyDrive/New_Project_pipeline2/Results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Accuracy table\n",
    "pd.DataFrame({\n",
    "    \"Model\": [\"Fusion (Speech + Text)\"],\n",
    "    \"Accuracy\": [acc_fusion]\n",
    "}).to_csv(os.path.join(results_dir, \"fusion_accuracy.csv\"), index=False)\n",
    "\n",
    "# Classification report\n",
    "with open(os.path.join(results_dir, \"fusion_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(classification_report(fusion_labels, fusion_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ijG18yZEAW4"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(fusion_labels, fusion_preds)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=emotion_names,\n",
    "            yticklabels=emotion_names)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Fusion Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"fusion_confusion_matrix.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTnnlgfgBFhl"
   },
   "outputs": [],
   "source": [
    "fusion_logits = []\n",
    "fusion_labels = []\n",
    "\n",
    "fusion_model.eval()\n",
    "\n",
    "for path, label in zip(test_paths, test_labels):\n",
    "\n",
    "    speech_emb = load_speech_test(path)\n",
    "    text_emb   = load_text_test(path)\n",
    "\n",
    "    if speech_emb is None or text_emb is None:\n",
    "        continue\n",
    "\n",
    "    speech_emb = speech_emb.to(device)\n",
    "    text_emb   = text_emb.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = fusion_model(speech_emb, text_emb)\n",
    "\n",
    "    fusion_logits.append(logits.cpu().numpy())\n",
    "    fusion_labels.append(label)\n",
    "\n",
    "fusion_logits = np.vstack(fusion_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "Y5qYUzQEC6Ol",
    "outputId": "1a98b6a0-75b7-4478-c2ff-f99115099bd4"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "logits_norm = scaler.fit_transform(fusion_logits)\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    n_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "fusion_2d = tsne.fit_transform(logits_norm)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for i, emo in enumerate(emotion_names):\n",
    "    idx = np.where(np.array(fusion_labels) == i)\n",
    "    plt.scatter(fusion_2d[idx,0], fusion_2d[idx,1], label=emo, s=50)\n",
    "\n",
    "plt.title(\"Fusion Decision Space (Logits) t-SNE\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(results_dir, \"fusion_tsne.png\"), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFmW59thBFbE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Thjd1pKVBE_u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
